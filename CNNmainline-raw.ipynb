{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df09a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /home/lunet/conn3/.local/lib/python3.9/site-packages (0.10.1)\n",
      "Requirement already satisfied: tensorflow in /home/lunet/conn3/.local/lib/python3.9/site-packages (2.13.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: numpy in /home/lunet/conn3/.local/lib/python3.9/site-packages (from mediapipe) (1.22.4)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda/lib/python3.9/site-packages (from mediapipe) (3.5.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda/lib/python3.9/site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: opencv-contrib-python in /home/lunet/conn3/.local/lib/python3.9/site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: absl-py in /home/lunet/conn3/.local/lib/python3.9/site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (1.56.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/anaconda/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda/lib/python3.9/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda/lib/python3.9/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/anaconda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/lunet/conn3/.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda/lib/python3.9/site-packages (from matplotlib->mediapipe) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda/lib/python3.9/site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda/lib/python3.9/site-packages (from matplotlib->mediapipe) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda/lib/python3.9/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda/lib/python3.9/site-packages (from matplotlib->mediapipe) (0.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe tensorflow scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1f2aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "C\n",
      "S\n",
      "F\n",
      "H\n",
      "S\n",
      "N\n",
      "S\n",
      "A\n",
      "H\n",
      "D\n",
      "D\n",
      "A\n",
      "H\n",
      "F\n",
      "N\n",
      "S\n",
      "S\n",
      "S\n",
      "H\n",
      "A\n",
      "N\n",
      "F\n",
      "H\n",
      "H\n",
      "D\n",
      "C\n",
      "N\n",
      "A\n",
      "D\n",
      "C\n",
      "F\n",
      "H\n",
      "N\n",
      "N\n",
      "S\n",
      "F\n",
      "S\n",
      "D\n",
      "S\n",
      "A\n",
      "H\n",
      "C\n",
      "F\n",
      "C\n",
      "A\n",
      "D\n",
      "H\n",
      "S\n",
      "H\n",
      "D\n",
      "C\n",
      "C\n",
      "A\n",
      "N\n",
      "S\n",
      "F\n",
      "F\n",
      "A\n",
      "N\n",
      "H\n",
      "D\n",
      "S\n",
      "C\n",
      "H\n",
      "F\n",
      "H\n",
      "H\n",
      "A\n",
      "A\n",
      "D\n",
      "S\n",
      "H\n",
      "H\n",
      "F\n",
      "D\n",
      "A\n",
      "S\n",
      "C\n",
      "D\n",
      "N\n",
      "F\n",
      "H\n",
      "F\n",
      "S\n",
      "H\n",
      "D\n",
      "H\n",
      "C\n",
      "S\n",
      "N\n",
      "H\n",
      "D\n",
      "S\n",
      "S\n",
      "S\n",
      "F\n",
      "N\n",
      "A\n",
      "C\n",
      "A\n",
      "N\n",
      "D\n",
      "H\n",
      "H\n",
      "C\n",
      "F\n",
      "A\n",
      "C\n",
      "F\n",
      "N\n",
      "A\n",
      "C\n",
      "F\n",
      "S\n",
      "S\n",
      "H\n",
      "N\n",
      "N\n",
      "F\n",
      "C\n",
      "S\n",
      "A\n",
      "C\n",
      "D\n",
      "F\n",
      "H\n",
      "S\n",
      "H\n",
      "H\n",
      "A\n",
      "N\n",
      "S\n",
      "D\n",
      "D\n",
      "S\n",
      "S\n",
      "A\n",
      "D\n",
      "S\n",
      "N\n",
      "C\n",
      "S\n",
      "C\n",
      "N\n",
      "A\n",
      "F\n",
      "S\n",
      "F\n",
      "D\n",
      "H\n",
      "H\n",
      "H\n",
      "A\n",
      "C\n",
      "N\n",
      "C\n",
      "A\n",
      "F\n",
      "H\n",
      "D\n",
      "H\n",
      "N\n",
      "F\n",
      "C\n",
      "S\n",
      "H\n",
      "S\n",
      "N\n",
      "A\n",
      "F\n",
      "H\n",
      "S\n",
      "A\n",
      "D\n",
      "C\n",
      "A\n",
      "D\n",
      "H\n",
      "C\n",
      "H\n",
      "F\n",
      "A\n",
      "H\n",
      "S\n",
      "S\n",
      "C\n",
      "D\n",
      "F\n",
      "N\n",
      "S\n",
      "C\n",
      "S\n",
      "F\n",
      "H\n",
      "D\n",
      "H\n",
      "A\n",
      "A\n",
      "H\n",
      "N\n",
      "A\n",
      "S\n",
      "H\n",
      "S\n",
      "N\n",
      "D\n",
      "S\n",
      "C\n",
      "D\n",
      "H\n",
      "N\n",
      "F\n",
      "C\n",
      "S\n",
      "H\n",
      "S\n",
      "N\n",
      "A\n",
      "F\n",
      "H\n",
      "S\n",
      "A\n",
      "D\n",
      "C\n",
      "A\n",
      "C\n",
      "S\n",
      "S\n",
      "H\n",
      "S\n",
      "A\n",
      "C\n",
      "S\n",
      "N\n",
      "C\n",
      "S\n",
      "H\n",
      "D\n",
      "H\n",
      "D\n",
      "A\n",
      "N\n",
      "F\n",
      "H\n",
      "F\n",
      "S\n",
      "H\n",
      "F\n",
      "C\n",
      "C\n",
      "D\n",
      "N\n",
      "N\n",
      "S\n",
      "A\n",
      "A\n",
      "D\n",
      "F\n",
      "H\n",
      "H\n",
      "S\n",
      "N\n",
      "C\n",
      "A\n",
      "D\n",
      "A\n",
      "N\n",
      "F\n",
      "C\n",
      "S\n",
      "A\n",
      "C\n",
      "D\n",
      "F\n",
      "H\n",
      "S\n",
      "H\n",
      "H\n",
      "A\n",
      "N\n",
      "S\n",
      "D\n",
      "H\n",
      "D\n",
      "S\n",
      "S\n",
      "S\n",
      "F\n",
      "N\n",
      "A\n",
      "C\n",
      "A\n",
      "N\n",
      "D\n",
      "H\n",
      "H\n",
      "C\n",
      "F\n",
      "N\n",
      "D\n",
      "F\n",
      "S\n",
      "C\n",
      "D\n",
      "S\n",
      "S\n",
      "A\n",
      "H\n",
      "F\n",
      "H\n",
      "A\n",
      "C\n",
      "N\n",
      "H\n",
      "H\n",
      "A\n",
      "H\n",
      "S\n",
      "D\n",
      "C\n",
      "S\n",
      "N\n",
      "F\n",
      "H\n",
      "F\n",
      "C\n",
      "D\n",
      "N\n",
      "S\n",
      "A\n",
      "S\n",
      "H\n",
      "A\n",
      "F\n",
      "H\n",
      "N\n",
      "S\n",
      "C\n",
      "A\n",
      "S\n",
      "N\n",
      "H\n",
      "H\n",
      "F\n",
      "S\n",
      "D\n",
      "D\n",
      "C\n",
      "A\n",
      "F\n",
      "D\n",
      "F\n",
      "A\n",
      "D\n",
      "S\n",
      "S\n",
      "H\n",
      "H\n",
      "H\n",
      "C\n",
      "N\n",
      "N\n",
      "S\n",
      "C\n",
      "F\n",
      "C\n",
      "A\n",
      "N\n",
      "D\n",
      "F\n",
      "S\n",
      "A\n",
      "S\n",
      "D\n",
      "S\n",
      "H\n",
      "H\n",
      "H\n",
      "C\n",
      "N\n",
      "H\n",
      "N\n",
      "D\n",
      "F\n",
      "H\n",
      "S\n",
      "A\n",
      "C\n",
      "H\n",
      "S\n",
      "S\n",
      "A\n",
      "F\n",
      "D\n",
      "C\n",
      "N\n",
      "H\n",
      "C\n",
      "N\n",
      "H\n",
      "F\n",
      "F\n",
      "S\n",
      "A\n",
      "D\n",
      "N\n",
      "D\n",
      "A\n",
      "C\n",
      "H\n",
      "S\n",
      "S\n",
      "S\n",
      "D\n",
      "A\n",
      "S\n",
      "A\n",
      "D\n",
      "N\n",
      "H\n",
      "N\n",
      "A\n",
      "S\n",
      "D\n",
      "C\n",
      "N\n",
      "S\n",
      "S\n",
      "H\n",
      "F\n",
      "F\n",
      "C\n",
      "H\n",
      "D\n",
      "A\n",
      "C\n",
      "S\n",
      "F\n",
      "C\n",
      "H\n",
      "H\n",
      "N\n",
      "F\n",
      "D\n",
      "D\n",
      "S\n",
      "H\n",
      "S\n",
      "A\n",
      "N\n",
      "A\n",
      "H\n",
      "H\n",
      "N\n",
      "S\n",
      "N\n",
      "F\n",
      "A\n",
      "D\n",
      "A\n",
      "C\n",
      "S\n",
      "D\n",
      "S\n",
      "H\n",
      "F\n",
      "C\n",
      "D\n",
      "H\n",
      "H\n",
      "A\n",
      "C\n",
      "F\n",
      "H\n",
      "S\n",
      "A\n",
      "N\n",
      "N\n",
      "S\n",
      "S\n",
      "C\n",
      "F\n",
      "D\n",
      "C\n",
      "H\n",
      "H\n",
      "S\n",
      "C\n",
      "F\n",
      "A\n",
      "D\n",
      "H\n",
      "A\n",
      "N\n",
      "S\n",
      "D\n",
      "F\n",
      "N\n",
      "S\n",
      "C\n",
      "S\n",
      "A\n",
      "H\n",
      "D\n",
      "S\n",
      "F\n",
      "C\n",
      "S\n",
      "N\n",
      "D\n",
      "H\n",
      "F\n",
      "A\n",
      "N\n",
      "H\n",
      "H\n",
      "H\n",
      "N\n",
      "S\n",
      "N\n",
      "F\n",
      "A\n",
      "D\n",
      "A\n",
      "C\n",
      "S\n",
      "D\n",
      "S\n",
      "H\n",
      "F\n",
      "C\n",
      "H\n",
      "A\n",
      "H\n",
      "D\n",
      "D\n",
      "C\n",
      "C\n",
      "S\n",
      "F\n",
      "C\n",
      "H\n",
      "H\n",
      "N\n",
      "F\n",
      "D\n",
      "D\n",
      "S\n",
      "H\n",
      "S\n",
      "A\n",
      "N\n",
      "A\n",
      "C\n",
      "H\n",
      "H\n",
      "S\n",
      "C\n",
      "F\n",
      "A\n",
      "D\n",
      "H\n",
      "A\n",
      "N\n",
      "S\n",
      "D\n",
      "F\n",
      "N\n",
      "S\n",
      "H\n",
      "N\n",
      "A\n",
      "S\n",
      "D\n",
      "C\n",
      "N\n",
      "S\n",
      "S\n",
      "H\n",
      "F\n",
      "F\n",
      "C\n",
      "H\n",
      "D\n",
      "A\n",
      "A\n",
      "C\n",
      "C\n",
      "D\n",
      "F\n",
      "H\n",
      "S\n",
      "N\n",
      "A\n",
      "H\n",
      "D\n",
      "S\n",
      "F\n",
      "S\n",
      "N\n",
      "H\n",
      "S\n",
      "A\n",
      "F\n",
      "N\n",
      "C\n",
      "H\n",
      "D\n",
      "D\n",
      "C\n",
      "S\n",
      "A\n",
      "N\n",
      "H\n",
      "F\n",
      "H\n",
      "N\n",
      "D\n",
      "H\n",
      "H\n",
      "S\n",
      "D\n",
      "C\n",
      "A\n",
      "N\n",
      "S\n",
      "D\n",
      "C\n",
      "A\n",
      "F\n",
      "H\n",
      "H\n",
      "D\n",
      "N\n",
      "S\n",
      "C\n",
      "H\n",
      "S\n",
      "F\n",
      "D\n",
      "H\n",
      "H\n",
      "A\n",
      "C\n",
      "F\n",
      "H\n",
      "S\n",
      "A\n",
      "N\n",
      "N\n",
      "S\n",
      "S\n",
      "C\n",
      "F\n",
      "D\n",
      "A\n",
      "F\n",
      "D\n",
      "F\n",
      "A\n",
      "D\n",
      "S\n",
      "S\n",
      "H\n",
      "H\n",
      "H\n",
      "C\n",
      "N\n",
      "N\n",
      "S\n",
      "C\n",
      "F\n",
      "C\n",
      "A\n",
      "N\n",
      "D\n",
      "F\n",
      "S\n",
      "A\n",
      "S\n",
      "D\n",
      "S\n",
      "H\n",
      "H\n",
      "H\n",
      "C\n",
      "N\n",
      "N\n",
      "F\n",
      "S\n",
      "H\n",
      "C\n",
      "A\n",
      "H\n",
      "H\n",
      "S\n",
      "H\n",
      "F\n",
      "S\n",
      "F\n",
      "D\n",
      "F\n",
      "S\n",
      "A\n",
      "H\n",
      "C\n",
      "H\n",
      "S\n",
      "H\n",
      "D\n",
      "N\n",
      "A\n",
      "S\n",
      "C\n",
      "N\n",
      "C\n",
      "N\n",
      "D\n",
      "F\n",
      "S\n",
      "C\n",
      "D\n",
      "S\n",
      "S\n",
      "A\n",
      "H\n",
      "F\n",
      "H\n",
      "A\n",
      "C\n",
      "N\n",
      "H\n",
      "S\n",
      "F\n",
      "F\n",
      "A\n",
      "H\n",
      "C\n",
      "H\n",
      "S\n",
      "H\n",
      "N\n",
      "A\n",
      "D\n",
      "A\n",
      "H\n",
      "F\n",
      "A\n",
      "F\n",
      "D\n",
      "S\n",
      "D\n",
      "N\n",
      "N\n",
      "H\n",
      "C\n",
      "C\n",
      "S\n",
      "S\n",
      "H\n",
      "H\n",
      "H\n",
      "S\n",
      "S\n",
      "F\n",
      "C\n",
      "H\n",
      "C\n",
      "D\n",
      "S\n",
      "S\n",
      "F\n",
      "H\n",
      "A\n",
      "N\n",
      "N\n",
      "S\n",
      "A\n",
      "H\n",
      "C\n",
      "D\n",
      "F\n",
      "F\n",
      "S\n",
      "C\n",
      "F\n",
      "H\n",
      "H\n",
      "N\n",
      "C\n",
      "D\n",
      "A\n",
      "D\n",
      "F\n",
      "S\n",
      "A\n",
      "C\n",
      "S\n",
      "N\n",
      "H\n",
      "S\n",
      "H\n",
      "C\n",
      "A\n",
      "F\n",
      "D\n",
      "S\n",
      "N\n",
      "S\n",
      "H\n",
      "N\n",
      "C\n",
      "A\n",
      "D\n",
      "F\n",
      "S\n",
      "H\n",
      "F\n",
      "D\n",
      "A\n",
      "N\n",
      "H\n",
      "H\n",
      "S\n",
      "C\n",
      "C\n",
      "S\n",
      "S\n",
      "A\n",
      "H\n",
      "F\n",
      "D\n",
      "N\n",
      "N\n",
      "S\n",
      "C\n",
      "S\n",
      "F\n",
      "A\n",
      "C\n",
      "H\n",
      "F\n",
      "H\n",
      "D\n",
      "N\n",
      "H\n",
      "A\n",
      "S\n",
      "D\n",
      "C\n",
      "D\n",
      "S\n",
      "A\n",
      "A\n",
      "H\n",
      "D\n",
      "F\n",
      "F\n",
      "S\n",
      "H\n",
      "H\n",
      "S\n",
      "N\n",
      "N\n",
      "C\n",
      "A\n",
      "S\n",
      "A\n",
      "D\n",
      "N\n",
      "D\n",
      "F\n",
      "C\n",
      "S\n",
      "F\n",
      "D\n",
      "S\n",
      "S\n",
      "A\n",
      "N\n",
      "D\n",
      "C\n",
      "H\n",
      "H\n",
      "A\n",
      "N\n",
      "A\n",
      "N\n",
      "N\n",
      "A\n",
      "S\n",
      "H\n",
      "F\n",
      "D\n",
      "C\n",
      "C\n",
      "S\n",
      "H\n",
      "S\n",
      "H\n",
      "D\n",
      "F\n",
      "F\n",
      "H\n",
      "A\n",
      "S\n",
      "C\n",
      "A\n",
      "F\n",
      "S\n",
      "H\n",
      "N\n",
      "D\n",
      "C\n",
      "H\n",
      "D\n",
      "S\n",
      "N\n",
      "D\n",
      "F\n",
      "N\n",
      "F\n",
      "S\n",
      "H\n",
      "A\n",
      "C\n",
      "D\n",
      "S\n",
      "A\n",
      "N\n",
      "S\n",
      "H\n",
      "H\n",
      "C\n",
      "D\n",
      "S\n",
      "S\n",
      "H\n",
      "A\n",
      "A\n",
      "C\n",
      "N\n",
      "H\n",
      "D\n",
      "C\n",
      "N\n",
      "F\n",
      "H\n",
      "S\n",
      "F\n",
      "N\n",
      "S\n",
      "A\n",
      "C\n",
      "H\n",
      "H\n",
      "C\n",
      "C\n",
      "H\n",
      "F\n",
      "N\n",
      "H\n",
      "D\n",
      "A\n",
      "S\n",
      "S\n",
      "D\n",
      "S\n",
      "N\n",
      "A\n",
      "F\n",
      "F\n",
      "A\n",
      "C\n",
      "D\n",
      "A\n",
      "D\n",
      "C\n",
      "S\n",
      "H\n",
      "S\n",
      "H\n",
      "H\n",
      "F\n",
      "N\n",
      "N\n",
      "D\n",
      "A\n",
      "S\n",
      "S\n",
      "C\n",
      "N\n",
      "N\n",
      "S\n",
      "F\n",
      "D\n",
      "H\n",
      "A\n",
      "F\n",
      "H\n",
      "C\n",
      "D\n",
      "H\n",
      "H\n",
      "C\n",
      "F\n",
      "S\n",
      "H\n",
      "N\n",
      "D\n",
      "S\n",
      "A\n",
      "H\n",
      "C\n",
      "C\n",
      "C\n",
      "S\n",
      "F\n",
      "H\n",
      "S\n",
      "N\n",
      "S\n",
      "A\n",
      "H\n",
      "D\n",
      "D\n",
      "A\n",
      "H\n",
      "F\n",
      "N\n",
      "H\n",
      "S\n",
      "H\n",
      "D\n",
      "C\n",
      "C\n",
      "A\n",
      "N\n",
      "S\n",
      "F\n",
      "F\n",
      "A\n",
      "N\n",
      "H\n",
      "D\n",
      "S\n",
      "H\n",
      "H\n",
      "S\n",
      "S\n",
      "D\n",
      "F\n",
      "H\n",
      "C\n",
      "D\n",
      "S\n",
      "S\n",
      "H\n",
      "C\n",
      "H\n",
      "N\n",
      "D\n",
      "A\n",
      "N\n",
      "S\n",
      "A\n",
      "F\n",
      "D\n",
      "S\n",
      "H\n",
      "C\n",
      "A\n",
      "F\n",
      "D\n",
      "S\n",
      "N\n",
      "S\n",
      "H\n",
      "N\n",
      "C\n",
      "A\n",
      "D\n",
      "F\n",
      "S\n",
      "H\n",
      "A\n",
      "H\n",
      "F\n",
      "A\n",
      "F\n",
      "D\n",
      "S\n",
      "D\n",
      "N\n",
      "N\n",
      "H\n",
      "C\n",
      "C\n",
      "S\n",
      "S\n",
      "H\n",
      "H\n",
      "C\n",
      "D\n",
      "S\n",
      "S\n",
      "F\n",
      "H\n",
      "A\n",
      "N\n",
      "N\n",
      "S\n",
      "A\n",
      "H\n",
      "C\n",
      "D\n",
      "F\n",
      "C\n",
      "S\n",
      "H\n",
      "S\n",
      "H\n",
      "C\n",
      "S\n",
      "F\n",
      "A\n",
      "N\n",
      "N\n",
      "F\n",
      "A\n",
      "H\n",
      "D\n",
      "S\n",
      "C\n",
      "D\n",
      "H\n",
      "H\n",
      "D\n",
      "H\n",
      "N\n",
      "D\n",
      "A\n",
      "S\n",
      "F\n",
      "S\n",
      "N\n",
      "C\n",
      "F\n",
      "S\n",
      "C\n",
      "A\n",
      "S\n",
      "S\n",
      "A\n",
      "N\n",
      "C\n",
      "S\n",
      "H\n",
      "N\n",
      "A\n",
      "F\n",
      "H\n",
      "D\n",
      "F\n",
      "H\n",
      "C\n",
      "D\n",
      "H\n",
      "H\n",
      "S\n",
      "H\n",
      "F\n",
      "S\n",
      "D\n",
      "S\n",
      "N\n",
      "C\n",
      "A\n",
      "A\n",
      "N\n",
      "F\n",
      "C\n",
      "D\n",
      "F\n",
      "F\n",
      "D\n",
      "N\n",
      "H\n",
      "N\n",
      "A\n",
      "S\n",
      "C\n",
      "H\n",
      "S\n",
      "D\n",
      "C\n",
      "S\n",
      "A\n",
      "H\n",
      "D\n",
      "S\n",
      "H\n",
      "H\n",
      "N\n",
      "A\n",
      "N\n",
      "A\n",
      "C\n",
      "H\n",
      "D\n",
      "C\n",
      "F\n",
      "S\n",
      "S\n",
      "F\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "def extract_word(string):\n",
    "    start_index = string.find('_') + 1  # Find the index of the underscore and add 1 to exclude it\n",
    "    end_index = string.find('.')  # Find the index of the dot\n",
    "\n",
    "    if start_index < end_index and start_index > 0:\n",
    "        return string[start_index:end_index]\n",
    "    else:\n",
    "        return None\n",
    "def extract_name(string):\n",
    "#     start_index = string.find('_') - 3  # Find the index of the underscore and add 1 to exclude it\n",
    "    end_index = string.find('_')  # Find the index of the dot\n",
    "\n",
    "    if 0 < end_index :\n",
    "        return string[0:end_index]\n",
    "    else:\n",
    "        return None\n",
    "def load_dataset(folder_path):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    people = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Add more extensions if needed\n",
    "                image_path = os.path.join(root, file)\n",
    "                label = os.path.basename(file)\n",
    "                name = extract_name(label)\n",
    "                label = extract_word(label)\n",
    "                \n",
    "                try:\n",
    "                    image = Image.open(image_path) \n",
    "                    image = cv2.resize(np.array(image,dtype=np.float32),(150,150)) / 255.0# Use PIL to open the image\n",
    "                    dataset.append(np.squeeze(image))\n",
    "                    labels.append(label[0])\n",
    "                    people.append(name)\n",
    "                except (IOError, OSError):\n",
    "                    print(f\"Error loading image: {image_path}\")\n",
    "\n",
    "    return dataset, labels, people\n",
    "\n",
    "# Example usage\n",
    "data_path = 'RADIATE_JPEGS1'\n",
    "dataset_folder = data_path\n",
    "images, labels, people = load_dataset(dataset_folder)\n",
    "\n",
    "# Access individual images and labels\n",
    "for image, label, person in zip(images, labels,people):\n",
    "    # image.show()  # Display the image using PIL\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeca4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'S', 'F', 'H', 'N', 'A', 'D']\n",
      "[154, 230, 147, 231, 146, 155, 153]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count of all the labels \n",
    "uniquelabels =[]\n",
    "countlabels = []\n",
    "id = 0\n",
    "\n",
    "for i in labels:\n",
    "  if i not in uniquelabels:\n",
    "    uniquelabels.append(i)\n",
    "for i in uniquelabels:\n",
    "  count = 0\n",
    "  for j in labels:\n",
    "    if i == j:\n",
    "      count += 1\n",
    "  countlabels.append(count)\n",
    "  id +=1\n",
    "print(uniquelabels)\n",
    "print(countlabels)\n",
    "sum(countlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafd8fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "truelabels = np.zeros(len(labels))\n",
    "for tag,i in enumerate(labels):\n",
    "    for id,j in enumerate(uniquelabels):\n",
    "        if i == j:\n",
    "            truelabels[tag]= id\n",
    "            print(tag)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = FaceMeshDetector()\n",
    "id = 0\n",
    "labels = []\n",
    "for img in images:\n",
    "    img = np.array(img)\n",
    "    results = mp_face_mesh.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Check if facial landmarks were detected\n",
    "    if results.multi_face_landmarks: \n",
    "        img, faces, nodes , ecount = detector.findFaceMesh(img)\n",
    "        #cv2.putText(img, f'Happiness', (10, 200), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 3)\n",
    "        #if len(faces )!= 0:\n",
    "            #print(len(faces))\n",
    "        if ecount == [0]:\n",
    "          # print('None')\n",
    "          ecount = 0\n",
    "        print(ecount)\n",
    "        label.append(ecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d01f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 21:44:12.950604: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 21:44:13.465409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2023-07-17 21:44:14.254374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-17 21:44:14.278595: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Load the MediaPipe face mesh model\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Load the FaceNet model\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "x = base_model.layers[-1].output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(x)\n",
    "base_model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Inputs\n",
    "image_input = layers.Input(shape=(224, 224, 3))\n",
    "landmarks_input = layers.Input(shape=(468, 2))\n",
    "\n",
    "# CNN for image processing\n",
    "image_features = base_model(image_input)\n",
    "\n",
    "# Flatten landmarks\n",
    "landmarks_flattened = layers.Flatten()(landmarks_input)\n",
    "\n",
    "# Concatenate image features and landmarks\n",
    "x = layers.concatenate([image_features, landmarks_flattened])\n",
    "\n",
    "# Fully connected layers\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output = layers.Dense(len(uniquelabels), activation='softmax')(x)  # Assuming num_classes is defined\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[image_input, landmarks_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95bab73f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float32') to dtype('uint8') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Assuming your dataset is stored in 'images' and 'labels' lists\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)):\n\u001b[0;32m---> 65\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_facial_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m landmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m         image, landmarks \u001b[38;5;241m=\u001b[39m preprocess(np\u001b[38;5;241m.\u001b[39marray(images[i]), landmarks)\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mextract_facial_landmarks\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     36\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Process the image and get the face landmarks\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mface_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/solutions/face_mesh.py:125\u001b[0m, in \u001b[0;36mFaceMesh.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    face landmarks on each detected face.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/solution_base.py:357\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    355\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    356\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m--> 357\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_stream_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/solution_base.py:589\u001b[0m, in \u001b[0;36mSolutionBase._make_packet\u001b[0;34m(self, packet_data_type, data)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_packet\u001b[39m(\u001b[38;5;28mself\u001b[39m, packet_data_type: PacketDataType,\n\u001b[1;32m    586\u001b[0m                  data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m packet\u001b[38;5;241m.\u001b[39mPacket:\n\u001b[1;32m    587\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (packet_data_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE_FRAME \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    588\u001b[0m       packet_data_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE):\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpacket_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreate_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpacket_data_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(packet_creator, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m packet_data_type\u001b[38;5;241m.\u001b[39mvalue)(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/packet_creator.py:245\u001b[0m, in \u001b[0;36mcreate_image\u001b[0;34m(data, image_format, copy)\u001b[0m\n\u001b[1;32m    241\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m is still writeable. Taking a reference of the data to create Image packet is dangerous.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_packet_creator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_image_from_pixel_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('float32') to dtype('uint8') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# # Load the MediaPipe face mesh model\n",
    "# face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# # Load the FaceNet model\n",
    "# base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "# x = base_model.layers[-1].output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# output = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(x)\n",
    "# model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.CosineSimilarity(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Function to extract facial landmarks using MediaPipe\n",
    "def extract_facial_landmarks(image):\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "        # Convert the image to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image and get the face landmarks\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        if results.multi_face_landmarks is None:\n",
    "            return None\n",
    "        \n",
    "        landmarks = []\n",
    "        for landmark in results.multi_face_landmarks[0].landmark:\n",
    "            landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "    return np.array(landmarks)\n",
    "\n",
    "# Function to preprocess image and landmarks\n",
    "def preprocess(image, landmarks):\n",
    "    # Resize and normalize image\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image / 255.0\n",
    "    \n",
    "    # Normalize landmarks to [-1, 1]\n",
    "    landmarks = (landmarks - image.shape[0] / 2) / (image.shape[0] / 2)\n",
    "    \n",
    "    return image, landmarks\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = []\n",
    "# Assuming your dataset is stored in 'images' and 'labels' lists\n",
    "for i in range(len(images)):\n",
    "    landmarks = extract_facial_landmarks(np.array(images[i]))\n",
    "    if landmarks is not None:\n",
    "        image, landmarks = preprocess(np.array(images[i]), landmarks)\n",
    "        label = truelabels[i]\n",
    "        dataset.append((image, landmarks, label))\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "train_set = dataset[:train_size]\n",
    "val_set = dataset[train_size:]\n",
    "\n",
    "# Prepare data for training\n",
    "X_train_img = np.array([item[0] for item in train_set])\n",
    "X_train_landmarks = np.array([item[1] for item in train_set])\n",
    "y_train = np.array([item[2] for item in train_set])\n",
    "\n",
    "X_val_img = np.array([item[0] for item in val_set])\n",
    "X_val_landmarks = np.array([item[1] for item in val_set])\n",
    "y_val = np.array([item[2] for item in val_set])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_img, X_train_landmarks], y_train, validation_data=([X_val_img, X_val_landmarks], y_val), epochs=100)\n",
    "\n",
    "# Make predictions\n",
    "# Assuming you have a test set 'X_test_img' and 'X_test_landmarks'\n",
    "predictions = model.predict([X_test_img, X_test_landmarks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e21d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Load the MediaPipe face mesh model\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)  # assuming 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to extract facial landmarks using MediaPipe\n",
    "def extract_facial_landmarks(image):\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "        # Convert the image to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image and get the face landmarks\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        if results.multi_face_landmarks is None:\n",
    "            return None\n",
    "\n",
    "        landmarks = []\n",
    "        for landmark in results.multi_face_landmarks[0].landmark:\n",
    "            landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "    return np.array(landmarks)\n",
    "\n",
    "# Define a custom dataset\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        landmarks = extract_facial_landmarks(image)\n",
    "        if landmarks is not None:\n",
    "            image, landmarks = preprocess(image, landmarks)\n",
    "            label = self.labels[idx]\n",
    "            return image, landmarks, label\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Function to preprocess image and landmarks\n",
    "def preprocess(image, landmarks):\n",
    "    # Resize and convert image to grayscale\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Normalize landmarks to [-1, 1]\n",
    "    landmarks = (landmarks - image.shape[1] / 2) / (image.shape[1] / 2)\n",
    "\n",
    "    return image, landmarks\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "# Assuming your dataset is stored in 'images' and 'labels' lists\n",
    "dataset = FaceDataset(images, labels)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_images, batch_landmarks, batch_labels in train_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_landmarks = batch_landmarks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_images)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_images, batch_landmarks, batch_labels in val_loader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_landmarks = batch_landmarks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Make predictions\n",
    "# Assuming you have a test set 'X_test_img' and 'X_test_landmarks'\n",
    "test_set = FaceDataset(X_test_img, [0] * len(X_test_img))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_images, batch_landmarks, _ in test_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_landmarks = batch_landmarks.to(device)\n",
    "        outputs = model(batch_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(predictions)\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Load the MediaPipe face mesh model\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Define the CNN architecture with attention mechanism\n",
    "class AttentionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 64)\n",
    "        self.attention = nn.Linear(64, 1)\n",
    "        self.fc2 = nn.Linear(64, 7)  # assuming 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attention_weights = nn.functional.softmax(self.attention(x), dim=1)\n",
    "        x = x * attention_weights\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to extract facial landmarks using MediaPipe\n",
    "def extract_facial_landmarks(image):\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "        # Convert the image to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image and get the face landmarks\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        if results.multi_face_landmarks is None:\n",
    "            return None\n",
    "\n",
    "        landmarks = []\n",
    "        for landmark in results.multi_face_landmarks[0].landmark:\n",
    "            landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "    return np.array(landmarks)\n",
    "\n",
    "# Define a custom dataset\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        landmarks = extract_facial_landmarks(image)\n",
    "        if landmarks is not None:\n",
    "            image, landmarks = preprocess(image, landmarks)\n",
    "            label = self.labels[idx]\n",
    "            return image, landmarks, label\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Function to preprocess image and landmarks\n",
    "def preprocess(image, landmarks):\n",
    "    # Resize and convert image to grayscale\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Normalize landmarks to [-1, 1]\n",
    "    landmarks = (landmarks - image.shape[1] / 2) / (image.shape[1] / 2)\n",
    "\n",
    "    return image, landmarks\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "# Assuming your dataset is stored in 'images' and 'labels' lists\n",
    "dataset = FaceDataset(images, labels)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = AttentionCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_images, batch_landmarks, batch_labels in train_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_landmarks = batch_landmarks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_images)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_images, batch_landmarks, batch_labels in val_loader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_landmarks = batch_landmarks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_images, batch_landmarks, batch_labels in test_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_landmarks = batch_landmarks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        outputs = model(batch_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3535d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 13:59:14.280155: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 13:59:19.919687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float32') to dtype('uint8') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    122\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_images, batch_landmarks, batch_labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    124\u001b[0m         batch_images \u001b[38;5;241m=\u001b[39m batch_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    125\u001b[0m         batch_landmarks \u001b[38;5;241m=\u001b[39m batch_landmarks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mFaceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     75\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[0;32m---> 76\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_facial_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m landmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         image, landmarks \u001b[38;5;241m=\u001b[39m preprocess(image, landmarks)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mextract_facial_landmarks\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     51\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Process the image and get the face landmarks\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mface_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/solutions/face_mesh.py:125\u001b[0m, in \u001b[0;36mFaceMesh.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    face landmarks on each detected face.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/solution_base.py:357\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    355\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    356\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m--> 357\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_stream_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/solution_base.py:589\u001b[0m, in \u001b[0;36mSolutionBase._make_packet\u001b[0;34m(self, packet_data_type, data)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_packet\u001b[39m(\u001b[38;5;28mself\u001b[39m, packet_data_type: PacketDataType,\n\u001b[1;32m    586\u001b[0m                  data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m packet\u001b[38;5;241m.\u001b[39mPacket:\n\u001b[1;32m    587\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (packet_data_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE_FRAME \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    588\u001b[0m       packet_data_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE):\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpacket_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreate_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpacket_data_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(packet_creator, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m packet_data_type\u001b[38;5;241m.\u001b[39mvalue)(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/mediapipe/python/packet_creator.py:245\u001b[0m, in \u001b[0;36mcreate_image\u001b[0;34m(data, image_format, copy)\u001b[0m\n\u001b[1;32m    241\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m is still writeable. Taking a reference of the data to create Image packet is dangerous.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_packet_creator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_image_from_pixel_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('float32') to dtype('uint8') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Load the MediaPipe face mesh model\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Define the CNN architecture with attention mechanism\n",
    "class AttentionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 64)\n",
    "        self.attention = nn.Linear(64, 1)\n",
    "        self.fc2 = nn.Linear(64, 7)  # assuming 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attention_weights = nn.functional.softmax(self.attention(x), dim=1)\n",
    "        x = x * attention_weights\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to extract facial landmarks using MediaPipe\n",
    "def extract_facial_landmarks(image):\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "        # Convert the image to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image and get the face landmarks\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        if results.multi_face_landmarks is None:\n",
    "            return None\n",
    "\n",
    "        landmarks = []\n",
    "        for landmark in results.multi_face_landmarks[0].landmark:\n",
    "            landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "    return np.array(landmarks)\n",
    "\n",
    "# Define a custom dataset\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        landmarks = extract_facial_landmarks(image)\n",
    "        if landmarks is not None:\n",
    "            image, landmarks = preprocess(image, landmarks)\n",
    "            label = self.labels[idx]\n",
    "            return image, landmarks, label\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Function to preprocess image and landmarks\n",
    "def preprocess(image, landmarks):\n",
    "    # Resize and convert image to grayscale\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Normalize landmarks to [-1, 1]\n",
    "    landmarks = (landmarks - image.shape[1] / 2) / (image.shape[1] / 2)\n",
    "\n",
    "    return image, landmarks\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "# Assuming your dataset is stored in 'images' and 'labels' lists\n",
    "dataset = FaceDataset(images, truelabels)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = AttentionCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_images, batch_landmarks, batch_labels in train_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_landmarks = batch_landmarks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_images)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_images, batch_landmarks, batch_labels in val_loader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_landmarks = batch_landmarks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_images, batch_landmarks, batch_labels in test_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_landmarks = batch_landmarks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        outputs = model(batch_images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture with attention mechanism\n",
    "class AttentionCNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AttentionCNN, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=3, strides=1)\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=3, strides=1)\n",
    "        self.conv3 = layers.Conv2D(128, kernel_size=3, strides=1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(64)\n",
    "        self.attention = layers.Dense(1)\n",
    "        self.fc2 = layers.Dense(7)  # assuming 7 output classes\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.nn.relu(self.conv1(x))\n",
    "        x = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2), padding='VALID')  # Fixed max_pool2d arguments and added padding\n",
    "        x = tf.nn.relu(self.conv2(x))\n",
    "        x = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2), padding='VALID')  # Fixed max_pool2d arguments and added padding\n",
    "        x = tf.nn.relu(self.conv3(x))\n",
    "        x = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2), padding='VALID')  # Fixed max_pool2d arguments and added padding\n",
    "        x = self.flatten(x)\n",
    "        x = tf.nn.relu(self.fc1(x))\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attention_weights = tf.nn.softmax(self.attention(x), axis=0)\n",
    "        x = tf.multiply(x, attention_weights)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create the model instance\n",
    "model = AttentionCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b3a58c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class LargerCNNWithAttention(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LargerCNNWithAttention, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.attention1 = tf.keras.layers.Conv2D(1, kernel_size=1, strides=1, activation='sigmoid')\n",
    "        self.conv4 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.attention2 = tf.keras.layers.Conv2D(1, kernel_size=1, strides=1, activation='sigmoid')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.attention3 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        self.fc2 = tf.keras.layers.Dense(7)  # Assuming 7 output classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        attention_map1 = self.attention1(x)\n",
    "        x = tf.multiply(x, attention_map1)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        attention_map2 = self.attention2(x)\n",
    "        x = tf.multiply(x, attention_map2)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        attention_weights = self.attention3(x)\n",
    "        x = tf.multiply(x, attention_weights)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the larger model with attention\n",
    "model = LargerCNNWithAttention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Specify which GPU to use, e.g., \"0\" for GPU 0\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Load the MediaPipe face mesh model\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Enable GPU growth to allocate memory on demand\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=0.1)\n",
    "\n",
    "# Define a custom training loop\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(inputs, training=True)\n",
    "        loss_value = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "\n",
    "# Create a custom dataset class\n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, labels, batch_size):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_images = self.images[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "# Prepare the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ffb13",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace with your label data\n",
    "\n",
    "# Normalize and reshape the images\n",
    "# images = cv2.resize(np.array(images),(150,150))\n",
    "# images = np.expand_dims(images, axis=-1)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels = np.array(truelabels)\n",
    "labels = to_categorical(labels)\n",
    "labels = tf.one_hot(labels, depth=7)\n",
    "labels = tf.reshape(labels, (-1,))  # Reshape labels to have shape (batch_size,)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "split_train = int(0.8 * len(images))\n",
    "split_val = int(0.9 * len(images))\n",
    "\n",
    "train_images, train_labels = images[:split_train], labels[:split_train]\n",
    "val_images, val_labels = images[split_train:split_val], labels[split_train:split_val]\n",
    "test_images, test_labels = images[split_val:], labels[split_val:]\n",
    "\n",
    "# Define batch size for train, validation, and test sets\n",
    "train_batch_size = 51\n",
    "val_batch_size = 51\n",
    "test_batch_size = 51\n",
    "\n",
    "# Create custom datasets for train, validation, and test\n",
    "train_dataset = CustomDataset(train_images, train_labels, train_batch_size)\n",
    "val_dataset = CustomDataset(val_images, val_labels, val_batch_size)\n",
    "test_dataset = CustomDataset(test_images, test_labels, test_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25df2fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(99862,), dtype=float32, numpy=array([0., 1., 0., ..., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d2744cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "Batch 1/32 - Loss: 0.4610\n",
      "Batch 2/32 - Loss: 0.4217\n",
      "Batch 3/32 - Loss: 0.4178\n",
      "Batch 4/32 - Loss: 0.4520\n",
      "Batch 5/32 - Loss: 0.4128\n",
      "Batch 6/32 - Loss: 0.4111\n",
      "Batch 7/32 - Loss: 0.4095\n",
      "Batch 8/32 - Loss: 0.4446\n",
      "Batch 9/32 - Loss: 0.4074\n",
      "Batch 10/32 - Loss: 0.4066\n",
      "Batch 11/32 - Loss: 0.4408\n",
      "Batch 12/32 - Loss: 0.4055\n",
      "Batch 13/32 - Loss: 0.4049\n",
      "Batch 14/32 - Loss: 0.4042\n",
      "Batch 15/32 - Loss: 0.4393\n",
      "Batch 16/32 - Loss: 0.4033\n",
      "Batch 17/32 - Loss: 0.4030\n",
      "Batch 18/32 - Loss: 0.4388\n",
      "Batch 19/32 - Loss: 0.4025\n",
      "Batch 20/32 - Loss: 0.4024\n",
      "Batch 21/32 - Loss: 0.4023\n",
      "Batch 22/32 - Loss: 0.4374\n",
      "Batch 23/32 - Loss: 0.4021\n",
      "Batch 24/32 - Loss: 0.4020\n",
      "Batch 25/32 - Loss: 0.4369\n",
      "Batch 26/32 - Loss: 0.4018\n",
      "Batch 27/32 - Loss: 0.4017\n",
      "Batch 28/32 - Loss: 0.4015\n",
      "Batch 29/32 - Loss: 0.4370\n",
      "Batch 30/32 - Loss: 0.4013\n",
      "Batch 31/32 - Loss: 0.4012\n",
      "Batch 32/32 - Loss: 0.4113\n",
      "Val Loss: 0.4100 - Val Accuracy: 43.7500\n",
      "Epoch 2/5000\n",
      "Batch 1/32 - Loss: 0.4368\n",
      "Batch 2/32 - Loss: 0.4012\n",
      "Batch 3/32 - Loss: 0.4013\n",
      "Batch 4/32 - Loss: 0.4360\n",
      "Batch 5/32 - Loss: 0.4014\n",
      "Batch 6/32 - Loss: 0.4013\n",
      "Batch 7/32 - Loss: 0.4011\n",
      "Batch 8/32 - Loss: 0.4364\n",
      "Batch 9/32 - Loss: 0.4009\n",
      "Batch 10/32 - Loss: 0.4008\n",
      "Batch 11/32 - Loss: 0.4366\n",
      "Batch 12/32 - Loss: 0.4008\n",
      "Batch 13/32 - Loss: 0.4009\n",
      "Batch 14/32 - Loss: 0.4008\n",
      "Batch 15/32 - Loss: 0.4361\n",
      "Batch 16/32 - Loss: 0.4009\n",
      "Batch 17/32 - Loss: 0.4009\n",
      "Batch 18/32 - Loss: 0.4359\n",
      "Batch 19/32 - Loss: 0.4009\n",
      "Batch 20/32 - Loss: 0.4009\n",
      "Batch 21/32 - Loss: 0.4007\n",
      "Batch 22/32 - Loss: 0.4362\n",
      "Batch 23/32 - Loss: 0.4006\n",
      "Batch 24/32 - Loss: 0.4006\n",
      "Batch 25/32 - Loss: 0.4362\n",
      "Batch 26/32 - Loss: 0.4007\n",
      "Batch 27/32 - Loss: 0.4008\n",
      "Batch 28/32 - Loss: 0.4007\n",
      "Batch 29/32 - Loss: 0.4359\n",
      "Batch 30/32 - Loss: 0.4007\n",
      "Batch 31/32 - Loss: 0.4007\n",
      "Batch 32/32 - Loss: 0.4107\n",
      "Val Loss: 0.4094 - Val Accuracy: 43.7500\n",
      "Epoch 3/5000\n",
      "Batch 1/32 - Loss: 0.4360\n",
      "Batch 2/32 - Loss: 0.4006\n",
      "Batch 3/32 - Loss: 0.4007\n",
      "Batch 4/32 - Loss: 0.4357\n",
      "Batch 5/32 - Loss: 0.4008\n",
      "Batch 6/32 - Loss: 0.4008\n",
      "Batch 7/32 - Loss: 0.4006\n",
      "Batch 8/32 - Loss: 0.4360\n",
      "Batch 9/32 - Loss: 0.4005\n",
      "Batch 10/32 - Loss: 0.4005\n",
      "Batch 11/32 - Loss: 0.4361\n",
      "Batch 12/32 - Loss: 0.4005\n",
      "Batch 13/32 - Loss: 0.4006\n",
      "Batch 14/32 - Loss: 0.4006\n",
      "Batch 15/32 - Loss: 0.4358\n",
      "Batch 16/32 - Loss: 0.4005\n",
      "Batch 17/32 - Loss: 0.4006\n",
      "Batch 18/32 - Loss: 0.4357\n",
      "Batch 19/32 - Loss: 0.4006\n",
      "Batch 20/32 - Loss: 0.4006\n",
      "Batch 21/32 - Loss: 0.4005\n",
      "Batch 22/32 - Loss: 0.4359\n",
      "Batch 23/32 - Loss: 0.4004\n",
      "Batch 24/32 - Loss: 0.4004\n",
      "Batch 25/32 - Loss: 0.4359\n",
      "Batch 26/32 - Loss: 0.4005\n",
      "Batch 27/32 - Loss: 0.4006\n",
      "Batch 28/32 - Loss: 0.4005\n",
      "Batch 29/32 - Loss: 0.4358\n",
      "Batch 30/32 - Loss: 0.4005\n",
      "Batch 31/32 - Loss: 0.4005\n",
      "Batch 32/32 - Loss: 0.4105\n",
      "Val Loss: 0.4092 - Val Accuracy: 43.7500\n",
      "Epoch 4/5000\n",
      "Batch 1/32 - Loss: 0.4359\n",
      "Batch 2/32 - Loss: 0.4005\n",
      "Batch 3/32 - Loss: 0.4005\n",
      "Batch 4/32 - Loss: 0.4355\n",
      "Batch 5/32 - Loss: 0.4006\n",
      "Batch 6/32 - Loss: 0.4006\n",
      "Batch 7/32 - Loss: 0.4005\n",
      "Batch 8/32 - Loss: 0.4358\n",
      "Batch 9/32 - Loss: 0.4003\n",
      "Batch 10/32 - Loss: 0.4003\n",
      "Batch 11/32 - Loss: 0.4360\n",
      "Batch 12/32 - Loss: 0.4004\n",
      "Batch 13/32 - Loss: 0.4005\n",
      "Batch 14/32 - Loss: 0.4004\n",
      "Batch 15/32 - Loss: 0.4357\n",
      "Batch 16/32 - Loss: 0.4004\n",
      "Batch 17/32 - Loss: 0.4004\n",
      "Batch 18/32 - Loss: 0.4356\n",
      "Batch 19/32 - Loss: 0.4005\n",
      "Batch 20/32 - Loss: 0.4005\n",
      "Batch 21/32 - Loss: 0.4004\n",
      "Batch 22/32 - Loss: 0.4359\n",
      "Batch 23/32 - Loss: 0.4003\n",
      "Batch 24/32 - Loss: 0.4003\n",
      "Batch 25/32 - Loss: 0.4358\n",
      "Batch 26/32 - Loss: 0.4004\n",
      "Batch 27/32 - Loss: 0.4005\n",
      "Batch 28/32 - Loss: 0.4004\n",
      "Batch 29/32 - Loss: 0.4356\n",
      "Batch 30/32 - Loss: 0.4004\n",
      "Batch 31/32 - Loss: 0.4004\n",
      "Batch 32/32 - Loss: 0.4104\n",
      "Val Loss: 0.4091 - Val Accuracy: 43.7500\n",
      "Epoch 5/5000\n",
      "Batch 1/32 - Loss: 0.4358\n",
      "Batch 2/32 - Loss: 0.4004\n",
      "Batch 3/32 - Loss: 0.4004\n",
      "Batch 4/32 - Loss: 0.4354\n",
      "Batch 5/32 - Loss: 0.4005\n",
      "Batch 6/32 - Loss: 0.4005\n",
      "Batch 7/32 - Loss: 0.4004\n",
      "Batch 8/32 - Loss: 0.4358\n",
      "Batch 9/32 - Loss: 0.4003\n",
      "Batch 10/32 - Loss: 0.4002\n",
      "Batch 11/32 - Loss: 0.4359\n",
      "Batch 12/32 - Loss: 0.4003\n",
      "Batch 13/32 - Loss: 0.4004\n",
      "Batch 14/32 - Loss: 0.4004\n",
      "Batch 15/32 - Loss: 0.4356\n",
      "Batch 16/32 - Loss: 0.4004\n",
      "Batch 17/32 - Loss: 0.4004\n",
      "Batch 18/32 - Loss: 0.4355\n",
      "Batch 19/32 - Loss: 0.4004\n",
      "Batch 20/32 - Loss: 0.4004\n",
      "Batch 21/32 - Loss: 0.4003\n",
      "Batch 22/32 - Loss: 0.4358\n",
      "Batch 23/32 - Loss: 0.4002\n",
      "Batch 24/32 - Loss: 0.4003\n",
      "Batch 25/32 - Loss: 0.4357\n",
      "Batch 26/32 - Loss: 0.4003\n",
      "Batch 27/32 - Loss: 0.4004\n",
      "Batch 28/32 - Loss: 0.4004\n",
      "Batch 29/32 - Loss: 0.4356\n",
      "Batch 30/32 - Loss: 0.4003\n",
      "Batch 31/32 - Loss: 0.4003\n",
      "Batch 32/32 - Loss: 0.4103\n",
      "Val Loss: 0.4091 - Val Accuracy: 43.7500\n",
      "Epoch 6/5000\n",
      "Batch 1/32 - Loss: 0.4357\n",
      "Batch 2/32 - Loss: 0.4003\n",
      "Batch 3/32 - Loss: 0.4004\n",
      "Batch 4/32 - Loss: 0.4353\n",
      "Batch 5/32 - Loss: 0.4005\n",
      "Batch 6/32 - Loss: 0.4005\n",
      "Batch 7/32 - Loss: 0.4003\n",
      "Batch 8/32 - Loss: 0.4357\n",
      "Batch 9/32 - Loss: 0.4002\n",
      "Batch 10/32 - Loss: 0.4002\n",
      "Batch 11/32 - Loss: 0.4358\n",
      "Batch 12/32 - Loss: 0.4003\n",
      "Batch 13/32 - Loss: 0.4003\n",
      "Batch 14/32 - Loss: 0.4003\n",
      "Batch 15/32 - Loss: 0.4355\n",
      "Batch 16/32 - Loss: 0.4003\n",
      "Batch 17/32 - Loss: 0.4003\n",
      "Batch 18/32 - Loss: 0.4355\n",
      "Batch 19/32 - Loss: 0.4004\n",
      "Batch 20/32 - Loss: 0.4004\n",
      "Batch 21/32 - Loss: 0.4003\n",
      "Batch 22/32 - Loss: 0.4357\n",
      "Batch 23/32 - Loss: 0.4002\n",
      "Batch 24/32 - Loss: 0.4002\n",
      "Batch 25/32 - Loss: 0.4356\n",
      "Batch 26/32 - Loss: 0.4003\n",
      "Batch 27/32 - Loss: 0.4004\n",
      "Batch 28/32 - Loss: 0.4003\n",
      "Batch 29/32 - Loss: 0.4355\n",
      "Batch 30/32 - Loss: 0.4003\n",
      "Batch 31/32 - Loss: 0.4003\n",
      "Batch 32/32 - Loss: 0.4103\n",
      "Val Loss: 0.4090 - Val Accuracy: 43.7500\n",
      "Epoch 7/5000\n",
      "Batch 1/32 - Loss: 0.4357\n",
      "Batch 2/32 - Loss: 0.4003\n",
      "Batch 3/32 - Loss: 0.4003\n",
      "Batch 4/32 - Loss: 0.4353\n",
      "Batch 5/32 - Loss: 0.4005\n",
      "Batch 6/32 - Loss: 0.4005\n",
      "Batch 7/32 - Loss: 0.4003\n",
      "Batch 8/32 - Loss: 0.4357\n",
      "Batch 9/32 - Loss: 0.4002\n",
      "Batch 10/32 - Loss: 0.4002\n",
      "Batch 11/32 - Loss: 0.4358\n",
      "Batch 12/32 - Loss: 0.4002\n",
      "Batch 13/32 - Loss: 0.4003\n",
      "Batch 14/32 - Loss: 0.4003\n",
      "Batch 15/32 - Loss: 0.4355\n",
      "Batch 16/32 - Loss: 0.4003\n",
      "Batch 17/32 - Loss: 0.4003\n",
      "Batch 18/32 - Loss: 0.4354\n",
      "Batch 19/32 - Loss: 0.4003\n",
      "Batch 20/32 - Loss: 0.4003\n",
      "Batch 21/32 - Loss: 0.4002\n",
      "Batch 22/32 - Loss: 0.4357\n",
      "Batch 23/32 - Loss: 0.4002\n",
      "Batch 24/32 - Loss: 0.4002\n",
      "Batch 25/32 - Loss: 0.4356\n",
      "Batch 26/32 - Loss: 0.4003\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4003\n",
      "Batch 29/32 - Loss: 0.4355\n",
      "Batch 30/32 - Loss: 0.4002\n",
      "Batch 31/32 - Loss: 0.4002\n",
      "Batch 32/32 - Loss: 0.4103\n",
      "Val Loss: 0.4090 - Val Accuracy: 43.7500\n",
      "Epoch 8/5000\n",
      "Batch 1/32 - Loss: 0.4357\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4003\n",
      "Batch 4/32 - Loss: 0.4352\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4003\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4358\n",
      "Batch 12/32 - Loss: 0.4002\n",
      "Batch 13/32 - Loss: 0.4003\n",
      "Batch 14/32 - Loss: 0.4003\n",
      "Batch 15/32 - Loss: 0.4355\n",
      "Batch 16/32 - Loss: 0.4003\n",
      "Batch 17/32 - Loss: 0.4003\n",
      "Batch 18/32 - Loss: 0.4354\n",
      "Batch 19/32 - Loss: 0.4003\n",
      "Batch 20/32 - Loss: 0.4003\n",
      "Batch 21/32 - Loss: 0.4002\n",
      "Batch 22/32 - Loss: 0.4357\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4002\n",
      "Batch 25/32 - Loss: 0.4356\n",
      "Batch 26/32 - Loss: 0.4003\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4003\n",
      "Batch 29/32 - Loss: 0.4355\n",
      "Batch 30/32 - Loss: 0.4002\n",
      "Batch 31/32 - Loss: 0.4002\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4090 - Val Accuracy: 43.7500\n",
      "Epoch 9/5000\n",
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4003\n",
      "Batch 4/32 - Loss: 0.4352\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4003\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4358\n",
      "Batch 12/32 - Loss: 0.4002\n",
      "Batch 13/32 - Loss: 0.4003\n",
      "Batch 14/32 - Loss: 0.4002\n",
      "Batch 15/32 - Loss: 0.4354\n",
      "Batch 16/32 - Loss: 0.4002\n",
      "Batch 17/32 - Loss: 0.4002\n",
      "Batch 18/32 - Loss: 0.4354\n",
      "Batch 19/32 - Loss: 0.4003\n",
      "Batch 20/32 - Loss: 0.4003\n",
      "Batch 21/32 - Loss: 0.4002\n",
      "Batch 22/32 - Loss: 0.4357\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4001\n",
      "Batch 25/32 - Loss: 0.4356\n",
      "Batch 26/32 - Loss: 0.4002\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4003\n",
      "Batch 29/32 - Loss: 0.4355\n",
      "Batch 30/32 - Loss: 0.4002\n",
      "Batch 31/32 - Loss: 0.4002\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4090 - Val Accuracy: 43.7500\n",
      "Epoch 10/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4003\n",
      "Batch 4/32 - Loss: 0.4352\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4002\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4357\n",
      "Batch 12/32 - Loss: 0.4002\n",
      "Batch 13/32 - Loss: 0.4002\n",
      "Batch 14/32 - Loss: 0.4002\n",
      "Batch 15/32 - Loss: 0.4354\n",
      "Batch 16/32 - Loss: 0.4002\n",
      "Batch 17/32 - Loss: 0.4002\n",
      "Batch 18/32 - Loss: 0.4354\n",
      "Batch 19/32 - Loss: 0.4003\n",
      "Batch 20/32 - Loss: 0.4003\n",
      "Batch 21/32 - Loss: 0.4002\n",
      "Batch 22/32 - Loss: 0.4357\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4001\n",
      "Batch 25/32 - Loss: 0.4355\n",
      "Batch 26/32 - Loss: 0.4002\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4002\n",
      "Batch 29/32 - Loss: 0.4354\n",
      "Batch 30/32 - Loss: 0.4002\n",
      "Batch 31/32 - Loss: 0.4002\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4090 - Val Accuracy: 43.7500\n",
      "Epoch 11/5000\n",
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4003\n",
      "Batch 4/32 - Loss: 0.4352\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4002\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4357\n",
      "Batch 12/32 - Loss: 0.4002\n",
      "Batch 13/32 - Loss: 0.4002\n",
      "Batch 14/32 - Loss: 0.4002\n",
      "Batch 15/32 - Loss: 0.4354\n",
      "Batch 16/32 - Loss: 0.4002\n",
      "Batch 17/32 - Loss: 0.4002\n",
      "Batch 18/32 - Loss: 0.4353\n",
      "Batch 19/32 - Loss: 0.4003\n",
      "Batch 20/32 - Loss: 0.4003\n",
      "Batch 21/32 - Loss: 0.4002\n",
      "Batch 22/32 - Loss: 0.4356\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4001\n",
      "Batch 25/32 - Loss: 0.4355\n",
      "Batch 26/32 - Loss: 0.4002\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4002\n",
      "Batch 29/32 - Loss: 0.4354\n",
      "Batch 30/32 - Loss: 0.4002\n",
      "Batch 31/32 - Loss: 0.4002\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4090 - Val Accuracy: 43.7500\n",
      "Epoch 12/5000\n",
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4003\n",
      "Batch 4/32 - Loss: 0.4352\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4002\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4357\n",
      "Batch 12/32 - Loss: 0.4001\n",
      "Batch 13/32 - Loss: 0.4002\n",
      "Batch 14/32 - Loss: 0.4002\n",
      "Batch 15/32 - Loss: 0.4354\n",
      "Batch 16/32 - Loss: 0.4002\n",
      "Batch 17/32 - Loss: 0.4002\n",
      "Batch 18/32 - Loss: 0.4353\n",
      "Batch 19/32 - Loss: 0.4002\n",
      "Batch 20/32 - Loss: 0.4002\n",
      "Batch 21/32 - Loss: 0.4002\n",
      "Batch 22/32 - Loss: 0.4356\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4001\n",
      "Batch 25/32 - Loss: 0.4355\n",
      "Batch 26/32 - Loss: 0.4002\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4002\n",
      "Batch 29/32 - Loss: 0.4354\n",
      "Batch 30/32 - Loss: 0.4002\n",
      "Batch 31/32 - Loss: 0.4002\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4089 - Val Accuracy: 43.7500\n",
      "Epoch 13/5000\n",
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4002\n",
      "Batch 4/32 - Loss: 0.4352\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4002\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4357\n",
      "Batch 12/32 - Loss: 0.4001\n",
      "Batch 13/32 - Loss: 0.4002\n",
      "Batch 14/32 - Loss: 0.4002\n",
      "Batch 15/32 - Loss: 0.4354\n",
      "Batch 16/32 - Loss: 0.4002\n",
      "Batch 17/32 - Loss: 0.4002\n",
      "Batch 18/32 - Loss: 0.4353\n",
      "Batch 19/32 - Loss: 0.4002\n",
      "Batch 20/32 - Loss: 0.4002\n",
      "Batch 21/32 - Loss: 0.4001\n",
      "Batch 22/32 - Loss: 0.4356\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4001\n",
      "Batch 25/32 - Loss: 0.4355\n",
      "Batch 26/32 - Loss: 0.4002\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4002\n",
      "Batch 29/32 - Loss: 0.4354\n",
      "Batch 30/32 - Loss: 0.4002\n",
      "Batch 31/32 - Loss: 0.4001\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4089 - Val Accuracy: 43.7500\n",
      "Epoch 14/5000\n",
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4002\n",
      "Batch 4/32 - Loss: 0.4351\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4002\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4357\n",
      "Batch 12/32 - Loss: 0.4001\n",
      "Batch 13/32 - Loss: 0.4002\n",
      "Batch 14/32 - Loss: 0.4002\n",
      "Batch 15/32 - Loss: 0.4354\n",
      "Batch 16/32 - Loss: 0.4002\n",
      "Batch 17/32 - Loss: 0.4002\n",
      "Batch 18/32 - Loss: 0.4353\n",
      "Batch 19/32 - Loss: 0.4002\n",
      "Batch 20/32 - Loss: 0.4002\n",
      "Batch 21/32 - Loss: 0.4001\n",
      "Batch 22/32 - Loss: 0.4356\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4001\n",
      "Batch 25/32 - Loss: 0.4355\n",
      "Batch 26/32 - Loss: 0.4002\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4002\n",
      "Batch 29/32 - Loss: 0.4354\n",
      "Batch 30/32 - Loss: 0.4001\n",
      "Batch 31/32 - Loss: 0.4001\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4089 - Val Accuracy: 43.7500\n",
      "Epoch 15/5000\n",
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4002\n",
      "Batch 4/32 - Loss: 0.4351\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4002\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4001\n",
      "Batch 11/32 - Loss: 0.4357\n",
      "Batch 12/32 - Loss: 0.4001\n",
      "Batch 13/32 - Loss: 0.4002\n",
      "Batch 14/32 - Loss: 0.4002\n",
      "Batch 15/32 - Loss: 0.4354\n",
      "Batch 16/32 - Loss: 0.4002\n",
      "Batch 17/32 - Loss: 0.4002\n",
      "Batch 18/32 - Loss: 0.4353\n",
      "Batch 19/32 - Loss: 0.4002\n",
      "Batch 20/32 - Loss: 0.4002\n",
      "Batch 21/32 - Loss: 0.4001\n",
      "Batch 22/32 - Loss: 0.4356\n",
      "Batch 23/32 - Loss: 0.4001\n",
      "Batch 24/32 - Loss: 0.4001\n",
      "Batch 25/32 - Loss: 0.4355\n",
      "Batch 26/32 - Loss: 0.4002\n",
      "Batch 27/32 - Loss: 0.4003\n",
      "Batch 28/32 - Loss: 0.4002\n",
      "Batch 29/32 - Loss: 0.4354\n",
      "Batch 30/32 - Loss: 0.4001\n",
      "Batch 31/32 - Loss: 0.4001\n",
      "Batch 32/32 - Loss: 0.4102\n",
      "Val Loss: 0.4089 - Val Accuracy: 43.7500\n",
      "Epoch 16/5000\n",
      "Batch 1/32 - Loss: 0.4356\n",
      "Batch 2/32 - Loss: 0.4002\n",
      "Batch 3/32 - Loss: 0.4002\n",
      "Batch 4/32 - Loss: 0.4351\n",
      "Batch 5/32 - Loss: 0.4004\n",
      "Batch 6/32 - Loss: 0.4004\n",
      "Batch 7/32 - Loss: 0.4002\n",
      "Batch 8/32 - Loss: 0.4356\n",
      "Batch 9/32 - Loss: 0.4001\n",
      "Batch 10/32 - Loss: 0.4000\n",
      "Batch 11/32 - Loss: 0.4357\n",
      "Batch 12/32 - Loss: 0.4001\n",
      "Batch 13/32 - Loss: 0.4002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[0;32m---> 37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize lists to store training and validation metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i, (inputs, labels) in enumerate(train_dataset):\n",
    "        loss = train_step(inputs, labels)\n",
    "        print(f\"Batch {i+1}/{len(train_dataset)} - Loss: {loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(val_dataset):\n",
    "        logits = model(inputs, training=False)\n",
    "\n",
    "        # Reshape labels to match logits shape\n",
    "        batch_size_actual = tf.shape(logits)[0]\n",
    "        labels_reshaped = tf.reshape(labels, [batch_size_actual, -1])\n",
    "\n",
    "        batch_loss = loss_fn(labels_reshaped, logits)\n",
    "        val_loss += batch_loss\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        predicted_labels = tf.argmax(logits, axis=-1)\n",
    "        correct_predictions = tf.reduce_sum(tf.cast(tf.equal(predicted_labels, tf.cast(labels_reshaped, tf.int64)), tf.int32))\n",
    "        val_correct += correct_predictions\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_correct = tf.reduce_sum(val_correct)\n",
    "\n",
    "    val_accuracy = val_correct / len(val_images)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss.numpy():.4f} - Val Accuracy: {val_accuracy.numpy():.4f}\")\n",
    "\n",
    "    # Append metrics to the lists\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracies.append(val_correct / len(train_images))\n",
    "    val_losses.append(val_loss.numpy())\n",
    "    val_accuracies.append(val_accuracy.numpy())\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a25a61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe684d94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'larger_cnn_with_attention_12' (type LargerCNNWithAttention).\n\nInput 0 of layer \"conv2d_76\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (51, 1404)\n\nCall arguments received by layer 'larger_cnn_with_attention_12' (type LargerCNNWithAttention):\n  • inputs=tf.Tensor(shape=(51, 1404), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m num_test_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dataset)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataset):\n\u001b[0;32m---> 37\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Reshape labels to match logits shape\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     batch_size_actual \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(logits)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36mLargerCNNWithAttention.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     attention_map1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention1(x)\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmultiply(x, attention_map1)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'larger_cnn_with_attention_12' (type LargerCNNWithAttention).\n\nInput 0 of layer \"conv2d_76\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (51, 1404)\n\nCall arguments received by layer 'larger_cnn_with_attention_12' (type LargerCNNWithAttention):\n  • inputs=tf.Tensor(shape=(51, 1404), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies, test_losses, test_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.plot(epochs, test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training, Validation, and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training, Validation, and Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Initialize lists to store test metrics\n",
    "test_losses = []\n",
    "test_correct = 0\n",
    "\n",
    "# Testing loop\n",
    "num_test_batches = len(test_dataset)\n",
    "for i, (inputs, labels) in enumerate(test_dataset):\n",
    "    logits = model(inputs, training=False)\n",
    "\n",
    "    # Reshape labels to match logits shape\n",
    "    batch_size_actual = tf.shape(logits)[0]\n",
    "    labels_reshaped = tf.reshape(labels, [batch_size_actual, -1])\n",
    "\n",
    "    batch_loss = loss_fn(labels_reshaped, logits)\n",
    "    test_losses.append(batch_loss)\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    predicted_labels = tf.argmax(logits, axis=-1)\n",
    "    correct_predictions = tf.reduce_sum(tf.cast(tf.equal(predicted_labels, tf.cast(labels_reshaped, tf.int64)), tf.int32))\n",
    "    test_correct += correct_predictions\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_loss = tf.reduce_sum(test_losses) / num_test_batches\n",
    "test_accuracy = test_correct / len(test_images)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Append test metrics to the lists\n",
    "test_losses_list = [loss.numpy() for loss in test_losses]\n",
    "test_accuracies_list = [test_correct / len(test_images)] * epochs\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies, test_losses_list, test_accuracies_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557a613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_dataset\u001b[49m):\n\u001b[1;32m     47\u001b[0m         loss \u001b[38;5;241m=\u001b[39m train_step(inputs, labels)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MediaPipe face mesh model\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Enable GPU growth to allocate memory on demand\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# Initialize the larger model with attention and Softmax activation for multi-class classification\n",
    "larger_model_with_attention = LargerCNNWithAttention()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Custom training loop\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = larger_model_with_attention(inputs, training=True)\n",
    "        loss_value = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss_value, larger_model_with_attention.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, larger_model_with_attention.trainable_variables))\n",
    "    return loss_value\n",
    "\n",
    "# Define the number of training epochs\n",
    "epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i, (inputs, labels) in enumerate(train_dataset):\n",
    "        loss = train_step(inputs, labels)\n",
    "        print(f\"Batch {i+1}/{len(train_dataset)} - Loss: {loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(val_dataset):\n",
    "        logits = larger_model_with_attention(inputs, training=False)\n",
    "        val_loss += loss_fn(labels, logits)\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        predicted_labels = tf.argmax(logits, axis=-1)\n",
    "        correct_predictions = tf.reduce_sum(tf.cast(tf.equal(predicted_labels, tf.argmax(labels, axis=-1)), tf.int32))\n",
    "        val_correct += correct_predictions\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_accuracy = val_correct / len(val_images)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "test_correct = 0\n",
    "num_test_batches = len(test_dataset)\n",
    "for i, (inputs, labels) in enumerate(test_dataset):\n",
    "    logits = larger_model_with_attention(inputs, training=False)\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    predicted_labels = tf.argmax(logits, axis=-1)\n",
    "    correct_predictions = tf.reduce_sum(tf.cast(tf.equal(predicted_labels, tf.argmax(labels, axis=-1)), tf.int32))\n",
    "    test_correct += correct_predictions\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = test_correct / len(test_images)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8326f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTFeatureExtractor, ViTForImageClassification\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load MediaPipe face mesh model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "# Load MediaPipe face mesh model\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Load pre-trained ViT model and feature extractor\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Prepare data and create custom datasets using TensorFlow Dataset\n",
    "# Use MediaPipe to extract features from your images\n",
    "def extract_features_from_image(image):\n",
    "    # Process the image using MediaPipe and extract features\n",
    "    # Example: Replace the following code with MediaPipe processing for your task\n",
    "    with mp_face_mesh.FaceMesh() as face_mesh:\n",
    "        # Convert image to RGB format\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image with MediaPipe\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        # Get the facial landmarks (example only, replace with relevant features for your task)\n",
    "        facial_landmarks = results.multi_face_landmarks\n",
    "\n",
    "    # Return the extracted features (example only, replace with relevant features for your task)\n",
    "    return facial_landmarks\n",
    "\n",
    "# Example function to prepare data and convert images to features\n",
    "def prepare_data(images, labels):\n",
    "    features = []\n",
    "    for image in images:\n",
    "        # Extract features using MediaPipe\n",
    "        features.append(extract_features_from_image(image))\n",
    "\n",
    "    # Convert the features to a suitable input format for the ViT model\n",
    "    # Example: Flatten the features and convert to NumPy array\n",
    "    features_flattened = [np.array(f).flatten() for f in features]\n",
    "\n",
    "    # Convert the features list to a NumPy array\n",
    "    features_array = np.array(features_flattened)\n",
    "\n",
    "    return features_array, labels\n",
    "\n",
    "# ...\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    for step, (inputs, labels) in enumerate(train_dataset):\n",
    "        # Forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(inputs, training=True).logits\n",
    "            loss = loss_fn(labels, logits)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Step {step+1}/{len(train_dataset)} - Loss: {loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for val_step, (val_inputs, val_labels) in enumerate(val_dataset):\n",
    "        logits = model(val_inputs, training=False).logits\n",
    "        val_loss += loss_fn(val_labels, logits)\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        predicted_labels = tf.argmax(logits, axis=-1)\n",
    "        correct_predictions = tf.reduce_sum(tf.cast(tf.equal(predicted_labels, tf.argmax(val_labels, axis=-1)), tf.int32))\n",
    "        val_correct += correct_predictions\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_accuracy = val_correct / len(val_images)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Testing, plotting learning curves, etc. (similar to your previous approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0dec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Specify which GPU to use, e.g., \"0\" for GPU 0\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MediaPipe face mesh model\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=0.1)\n",
    "\n",
    "# Custom Dataset class with MediaPipe feature extraction\n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, labels, batch_size):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_images = self.images[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "\n",
    "        # Extract features using MediaPipe for each image\n",
    "        batch_features = [extract_features_from_image(img) for img in batch_images]\n",
    "        batch_features = np.array(batch_features)\n",
    "        batch_features = tf.reshape(batch_features, (243, 468, 3, 1))\n",
    "        return batch_features, np.array(batch_labels)\n",
    "\n",
    "# Function to extract features using MediaPipe from each image\n",
    "def extract_features_from_image(image):\n",
    "    # Convert image to RGB format\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert image to uint8 format\n",
    "    image_uint8 = np.clip(image_rgb * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Process the image with MediaPipe\n",
    "    results = face_mesh.process(image_uint8)\n",
    "\n",
    "    # Get the facial landmarks for the first face (if any)\n",
    "    if results.multi_face_landmarks:\n",
    "        facial_landmarks = results.multi_face_landmarks[0]\n",
    "        # Process the facial landmarks (example only, replace with relevant features for your task)\n",
    "        features = []\n",
    "        for landmark in facial_landmarks.landmark:\n",
    "            features.extend([landmark.x, landmark.y, landmark.z])\n",
    "        return features\n",
    "    else:\n",
    "        # Return None if no face landmarks are detected\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c293d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace with your label data\n",
    "\n",
    "# Normalize and reshape the images\n",
    "# images = cv2.resize(np.array(images),(150,150))\n",
    "# images = np.expand_dims(images, axis=-1)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels = np.array(truelabels)\n",
    "labels = to_categorical(labels)\n",
    "labels = tf.one_hot(labels, depth=7)\n",
    "labels = tf.reshape(labels, (-1,))  # Reshape labels to have shape (batch_size,)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "split_train = int(0.8 * len(images))\n",
    "split_val = int(0.9 * len(images))\n",
    "\n",
    "train_images, train_labels = images[:split_train], labels[:split_train]\n",
    "val_images, val_labels = images[split_train:split_val], labels[split_train:split_val]\n",
    "test_images, test_labels = images[split_val:], labels[split_val:]\n",
    "\n",
    "# Define batch size for train, validation, and test sets\n",
    "train_batch_size = 51\n",
    "val_batch_size = 51\n",
    "test_batch_size = 51\n",
    "\n",
    "# Create custom datasets for train, validation, and test\n",
    "train_dataset = CustomDataset(train_images, train_labels, train_batch_size)\n",
    "val_dataset = CustomDataset(val_images, val_labels, val_batch_size)\n",
    "test_dataset = CustomDataset(test_images, test_labels, test_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b088c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "20a38208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the attention layer as a custom layer\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention_weights = self.add_weight(\n",
    "            shape=(1, 1, 1, input_shape[-1]),\n",
    "            initializer=tf.keras.initializers.ones(),\n",
    "            trainable=True,\n",
    "            name=\"attention_weights\",\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_scores = tf.nn.softmax(tf.multiply(inputs, self.attention_weights), axis=-1)\n",
    "        return tf.multiply(inputs, attention_scores)\n",
    "\n",
    "\n",
    "# Larger CNN with Attention and MediaPipe feature extraction\n",
    "class LargerCNNWithAttention(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LargerCNNWithAttention, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.attention1 = AttentionLayer()\n",
    "        self.conv4 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.attention2 = AttentionLayer()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.attention3 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        self.fc2 = tf.keras.layers.Dense(7)  # Assuming 7 output classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        attention_map1 = self.attention1(x)\n",
    "        x = tf.multiply(x, attention_map1)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        attention_map2 = self.attention2(x)\n",
    "        x = tf.multiply(x, attention_map2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        attention_weights = self.attention3(x)\n",
    "        x = tf.expand_dims(x, axis=-1)  # Add a new dimension to match the attention_weights shape\n",
    "\n",
    "        x = tf.multiply(x, attention_weights)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = LargerCNNWithAttention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a091ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class LargerCNNWithAttention(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LargerCNNWithAttention, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.attention1 = tf.keras.layers.Conv2D(1, kernel_size=1, strides=1, activation='sigmoid')\n",
    "        self.conv4 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding='same', activation='relu')\n",
    "        self.attention2 = tf.keras.layers.Conv2D(1, kernel_size=1, strides=1, activation='sigmoid')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.attention3 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        self.fc2 = tf.keras.layers.Dense(7)  # Assuming 7 output classes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        attention_map1 = self.attention1(x)\n",
    "        x = tf.multiply(x, attention_map1)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        attention_map2 = self.attention2(x)\n",
    "        x = tf.multiply(x, attention_map2)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        attention_weights = self.attention3(x)\n",
    "        x = tf.multiply(x, attention_weights)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the larger model with attention\n",
    "model = LargerCNNWithAttention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be234a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a6e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b16de87",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 71604 values, but the requested shape has 341172 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m [extract_features_from_image(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m batch_images]\n\u001b[1;32m     36\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(batch_features)\n\u001b[0;32m---> 37\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m243\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m468\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_features, np\u001b[38;5;241m.\u001b[39marray(batch_labels)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 71604 values, but the requested shape has 341172 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "model.fit(train_dataset, batch_size=batch_size, epochs=epochs, validation_data=(val_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7333f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"larger_cnn_with_attention_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_82 (Conv2D)          multiple                  320       \n",
      "                                                                 \n",
      " conv2d_83 (Conv2D)          multiple                  36992     \n",
      "                                                                 \n",
      " conv2d_84 (Conv2D)          multiple                  295168    \n",
      "                                                                 \n",
      " conv2d_85 (Conv2D)          multiple                  33        \n",
      "                                                                 \n",
      " conv2d_86 (Conv2D)          multiple                  1180160   \n",
      "                                                                 \n",
      " conv2d_87 (Conv2D)          multiple                  129       \n",
      "                                                                 \n",
      " flatten_18 (Flatten)        multiple                  0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            multiple                  1024      \n",
      "                                                                 \n",
      " dense_55 (Dense)            multiple                  1025      \n",
      "                                                                 \n",
      " dense_56 (Dense)            multiple                  7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1522026 (5.81 MB)\n",
      "Trainable params: 1522026 (5.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27b15846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_591723/1049821819.py:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch_features = np.array(batch_features)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,j \u001b[38;5;129;01min\u001b[39;00m test_dataset:\n\u001b[1;32m      3\u001b[0m     pred\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mpredict(i))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/data_utils.py:567\u001b[0m, in \u001b[0;36mSequence.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/data_utils.py:567\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m [extract_features_from_image(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m batch_images]\n\u001b[1;32m     36\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(batch_features)\n\u001b[0;32m---> 37\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m243\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m468\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_features, np\u001b[38;5;241m.\u001b[39marray(batch_labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for i,j in test_dataset:\n",
    "    pred.append(model.predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11ba8fe6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_591723/3384534511.py\", line 6, in train_step  *\n        logits = model(inputs, training=True)\n    File \"/home/lunet/conn3/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filea_veo07b.py\", line 17, in tf__call\n        x = ag__.converted_call(ag__.converted_call(ag__.ld(tf).keras.layers.MaxPooling2D, (2,), None, fscope), (ag__.ld(x),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'larger_cnn_with_attention_15' (type LargerCNNWithAttention).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_591723/3502636233.py\", line 26, in call  *\n            x = tf.keras.layers.MaxPooling2D(2)(x)\n        File \"/home/lunet/conn3/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'max_pooling2d_1' (type MaxPooling2D).\n        \n        Negative dimension size caused by subtracting 2 from 1 for '{{node larger_cnn_with_attention_15/max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](larger_cnn_with_attention_15/Mul_1)' with input shapes: [51,234,1,128].\n        \n        Call arguments received by layer 'max_pooling2d_1' (type MaxPooling2D):\n          • inputs=tf.Tensor(shape=(51, 234, 1, 128), dtype=float32)\n    \n    \n    Call arguments received by layer 'larger_cnn_with_attention_15' (type LargerCNNWithAttention):\n      • inputs=tf.Tensor(shape=(51, 468, 3, 1), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[0;32m---> 46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file7puhrzw4.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(inputs, labels)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     12\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(loss_fn), (ag__\u001b[38;5;241m.\u001b[39mld(labels), ag__\u001b[38;5;241m.\u001b[39mld(logits)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss_value), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filea_veo07b.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m attention_map2 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mattention2, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmultiply, (ag__\u001b[38;5;241m.\u001b[39mld(x), ag__\u001b[38;5;241m.\u001b[39mld(attention_map2)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling2D, (\u001b[38;5;241m2\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconv3, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling2D, (\u001b[38;5;241m2\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_591723/3384534511.py\", line 6, in train_step  *\n        logits = model(inputs, training=True)\n    File \"/home/lunet/conn3/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filea_veo07b.py\", line 17, in tf__call\n        x = ag__.converted_call(ag__.converted_call(ag__.ld(tf).keras.layers.MaxPooling2D, (2,), None, fscope), (ag__.ld(x),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'larger_cnn_with_attention_15' (type LargerCNNWithAttention).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_591723/3502636233.py\", line 26, in call  *\n            x = tf.keras.layers.MaxPooling2D(2)(x)\n        File \"/home/lunet/conn3/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'max_pooling2d_1' (type MaxPooling2D).\n        \n        Negative dimension size caused by subtracting 2 from 1 for '{{node larger_cnn_with_attention_15/max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](larger_cnn_with_attention_15/Mul_1)' with input shapes: [51,234,1,128].\n        \n        Call arguments received by layer 'max_pooling2d_1' (type MaxPooling2D):\n          • inputs=tf.Tensor(shape=(51, 234, 1, 128), dtype=float32)\n    \n    \n    Call arguments received by layer 'larger_cnn_with_attention_15' (type LargerCNNWithAttention):\n      • inputs=tf.Tensor(shape=(51, 468, 3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Custom training loop\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(inputs, training=True)\n",
    "        loss_value = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss_value\n",
    "\n",
    "def plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize lists to store training and validation metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i, (inputs, labels) in enumerate(train_dataset):\n",
    "        loss = train_step(inputs, labels)\n",
    "        print(f\"Batch {i+1}/{len(train_dataset)} - Loss: {loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(val_dataset):\n",
    "        logits = model(inputs, training=False)\n",
    "\n",
    "        # Reshape labels to match logits shape\n",
    "        batch_size_actual = tf.shape(logits)[0]\n",
    "        labels_reshaped = tf.reshape(labels, [batch_size_actual, -1])\n",
    "\n",
    "        batch_loss = loss_fn(labels_reshaped, logits)\n",
    "        val_loss += batch_loss\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        predicted_labels = tf.argmax(logits, axis=-1)\n",
    "        correct_predictions = tf.reduce_sum(tf.cast(tf.equal(predicted_labels, tf.cast(labels_reshaped, tf.int64)), tf.int32))\n",
    "        val_correct += correct_predictions\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_correct = tf.reduce_sum(val_correct)\n",
    "\n",
    "    val_accuracy = val_correct / len(val_images)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss.numpy():.4f} - Val Accuracy: {val_accuracy.numpy():.4f}\")\n",
    "\n",
    "    # Append metrics to the lists\n",
    "    train_losses.append(loss.numpy())\n",
    "    train_accuracies.append(val_correct / len(train_images))\n",
    "    val_losses.append(val_loss.numpy())\n",
    "    val_accuracies.append(val_accuracy.numpy())\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2662a8bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 71604 values, but the requested shape has 341172 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, (i,j) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(i)\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;28mid\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/data_utils.py:567\u001b[0m, in \u001b[0;36mSequence.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/data_utils.py:567\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m [extract_features_from_image(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m batch_images]\n\u001b[1;32m     36\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(batch_features)\n\u001b[0;32m---> 37\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m243\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m468\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_features, np\u001b[38;5;241m.\u001b[39marray(batch_labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 71604 values, but the requested shape has 341172 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "for id, (i,j) in enumerate(train_dataset):\n",
    "    print(np.array(i).shape,id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6cef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
